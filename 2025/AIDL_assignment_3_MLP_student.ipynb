{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DQtTcuGoDlH7"},"source":["## AIDL Deep Learning and Neural Networks\n","### Columbia University\n"]},{"cell_type":"markdown","metadata":{"id":"Qe6yTh55trpQ"},"source":["# Assignment 3: Multilayer Perceptron (MLP)\n","In this assignment, you will be implementing an MLP using TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"D8FIq8btOU8e"},"source":["#### import packages"]},{"cell_type":"code","metadata":{"id":"7Vs2WYIFtrpS","tags":[],"executionInfo":{"status":"ok","timestamp":1730947836597,"user_tz":300,"elapsed":147,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["# Import modules\n","from __future__ import print_function\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import os\n","import matplotlib.pyplot as plt\n","import pickle\n","import tarfile\n","import glob\n","import urllib.request as url\n","# from utils.cifar_utils import load_data\n","\n","# Plot configurations\n","%matplotlib inline\n","\n","# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WinZKOQTOc23"},"source":["#### Functions for downloading CIFAR-100 dataset"]},{"cell_type":"code","metadata":{"id":"6QIrz7dLFP39","executionInfo":{"status":"ok","timestamp":1730947838557,"user_tz":300,"elapsed":250,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["def unpickle(file):\n","    with open(file, 'rb') as fo:\n","        res = pickle.load(fo, encoding='bytes')\n","    return res\n","\n","\n","def download_data():\n","    \"\"\"\n","    Download the CIFAR-100 data from the website, which is approximately 170MB.\n","    The data (a .tar.gz file) will be store in the ./data/ folder.\n","    :return: None\n","    \"\"\"\n","    if not os.path.exists('./data'):\n","        os.mkdir('./data')\n","        print('Start downloading data...')\n","        url.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\",\n","                        \"./data/cifar-100-python.tar.gz\")\n","        print('Download complete.')\n","    else:\n","        if os.path.exists('./data/cifar-100-python.tar.gz'):\n","            print('CIFAR-100 package already exists.')\n","        print('Start downloading data...')\n","        url.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\",\n","                        \"./data/cifar-100-python.tar.gz\")\n","        print('Download complete.')\n","\n","\n","\n","def load_data():\n","    \"\"\"\n","    Unpack the CIFAR-100 dataset and load the coarse datasets (20-class).\n","    :return: A tuple of label_map, data/labels. For both training and test sets.\n","    \"\"\"\n","    # If the data hasn't been downloaded yet, download it first.\n","    if not os.path.exists('./data/cifar-100-python.tar.gz'):\n","        download_data()\n","    # Check if the package has been unpacked, otherwise unpack the package\n","    if not os.path.exists('./data/cifar-100-python/'):\n","        package = tarfile.open('./data/cifar-100-python.tar.gz')\n","        package.extractall('./data')\n","        package.close()\n","\n","    # Go to the location where the files are unpacked\n","    os.chdir('./')\n","\n","    # load the label_map and data\n","    meta = unpickle('./data/cifar-100-python/meta')\n","\n","    coarse_label_names = [t.decode('utf8') for t in meta[b'coarse_label_names']]\n","\n","    train = unpickle('./data/cifar-100-python//train')\n","    y_train = np.array(train[b'coarse_labels'])\n","    X_train = np.array(train[b'data'])\n","\n","    test = unpickle('./data/cifar-100-python//test')\n","    y_test = np.array(test[b'coarse_labels'])\n","    X_test = np.array(test[b'data'])\n","\n","    return coarse_label_names, X_train, y_train, X_test, y_test"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gYnTjputrpV"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I31uJ6KltrpW","tags":[],"outputId":"1829c14b-d897-4ce2-abea-6776e5c943e6","executionInfo":{"status":"ok","timestamp":1730947843338,"user_tz":300,"elapsed":2201,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["# Load the raw CIFAR-100 data.\n","label_map, X_train, y_train, X_test, y_test = load_data()\n","\n","# Data organizations:\n","# Train data: 49000 samples from original train set: 1~49,000\n","# Validation data: 1000 samples from original train set: 49,000~50,000\n","# Test data: 10000 samples from original test set: 1~10,000\n","# Development data (for gradient check): 100 from the train set: 1~49,000\n","num_training = 49000\n","num_validation = 1000\n","num_dev = 100\n","\n","X_val = X_train[-num_validation:, :]\n","y_val = y_train[-num_validation:]\n","\n","mask = np.random.choice(num_training, num_dev, replace=False)\n","X_dev = X_train[mask]\n","y_dev = y_train[mask]\n","\n","X_train = X_train[:num_training, :]\n","y_train = y_train[:num_training]\n","\n","# Preprocessing: subtract the mean value across every dimension for training data\n","mean_image = np.mean(X_train, axis=0)\n","\n","X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n","X_val = X_val.astype(np.float32) - mean_image\n","X_test = X_test.astype(np.float32) - mean_image\n","X_dev = X_dev.astype(np.float32) - mean_image\n","\n","print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('Development data shape:', X_dev.shape)\n","print('Development data shape', y_dev.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(49000, 3072) (1000, 3072) (10000, 3072) (100, 3072)\n","Train data shape:  (49000, 3072)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3072)\n","Validation labels shape:  (1000,)\n","Test data shape:  (10000, 3072)\n","Test labels shape:  (10000,)\n","Development data shape: (100, 3072)\n","Development data shape (100,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xa_w8ZHbtrqY"},"source":["## Part 1: TensorFlow MLP\n","In this part, you will use TensorFlow modules to implement an MLP. We provide a demo of a two-layer net; for more information, look at https://www.tensorflow.org/guide/keras and https://www.tensorflow.org/guide/eager.\n","\n","You will need to implement a multi-layer network with 3 layers in a similar style."]},{"cell_type":"markdown","metadata":{"id":"kpHQ-jOzDlIL"},"source":["### Demo: Two-layer MLP in Tensorflow"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0xQOzpdtrqZ","tags":[],"outputId":"0f498abc-c18b-4dac-804d-bd0963f6cfc2","executionInfo":{"status":"ok","timestamp":1730947939296,"user_tz":300,"elapsed":92301,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["## Demo: Two-layer net in tensorflow (eager execution mode)\n","hidden_dim = 300\n","reg_tf = tf.constant(0.01)\n","\n","# define a tf.keras.Model class\n","class Model(tf.keras.Model):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.W1 = tf.Variable(1e-2*np.random.rand(3072, hidden_dim).astype('float32'))\n","        self.b1 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n","        self.W2 = tf.Variable(1e-2*np.random.rand(hidden_dim, 20).astype('float32'))\n","        self.b2 = tf.Variable(np.zeros((20,)).astype('float32'))\n","    def call(self, inputs):\n","        \"\"\"Run the model.\"\"\"\n","        h1 = tf.nn.relu(tf.matmul(inputs, self.W1) + self.b1)\n","        out = tf.matmul(h1, self.W2) + self.b2\n","        return out\n","\n","# Define and calculate loss function (Note that in eager execution, loss must be in a function)\n","def loss(model, inputs, targets, reg = tf.constant(0.01)):\n","    out = model(inputs)\n","    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= out, labels=tf.one_hot(targets,20))\n","    L2_loss = tf.nn.l2_loss(model.W1) + tf.nn.l2_loss(model.W2)\n","    return tf.reduce_mean(cross_entropy) + reg * L2_loss\n","\n","# calculate gradients for all variables using tf.GradientTape\n","def grad(model, inputs, targets, reg = tf.constant(0.01)):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss(model, inputs, targets, reg=reg)\n","    return tape.gradient(loss_value, [model.W1, model.b1, model.W2, model.b2])\n","\n","# calculate classification accuracy\n","def eval_acc(model, inputs, targets):\n","    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    return accuracy\n","\n","num_train = 49000\n","batch_size = 500\n","num_batch = num_train//batch_size\n","num_epochs = 12\n","model = Model()\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n","\n","for e in range(num_epochs):\n","    for i in range(num_batch):\n","        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n","        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n","        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n","\n","        grads = grad(model, x_tf, y_tf, reg_tf)\n","        #optimization based on calculated gradients\n","        optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2]))\n","\n","    x_tf = tf.Variable(X_val, dtype = tf.float32)\n","    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n","    accuracy = eval_acc(model, x_tf, y_tf)\n","    val_acc = accuracy.numpy()\n","    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n","\n","x_tf = tf.Variable(X_test, dtype = tf.float32)\n","y_tf = tf.Variable(y_test, dtype = tf.uint8)\n","accuracy = eval_acc(model, x_tf, y_tf)\n","test_acc = accuracy.numpy()\n","print('test acc = {}'.format(test_acc))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: valid acc = 0.0860000029206276\n","epoch 2: valid acc = 0.10400000214576721\n","epoch 3: valid acc = 0.0989999994635582\n","epoch 4: valid acc = 0.14499999582767487\n","epoch 5: valid acc = 0.12600000202655792\n","epoch 6: valid acc = 0.12600000202655792\n","epoch 7: valid acc = 0.15000000596046448\n","epoch 8: valid acc = 0.15700000524520874\n","epoch 9: valid acc = 0.16099999845027924\n","epoch 10: valid acc = 0.1770000010728836\n","epoch 11: valid acc = 0.20600000023841858\n","epoch 12: valid acc = 0.2070000022649765\n","test acc = 0.21969999372959137\n"]}]},{"cell_type":"markdown","metadata":{"id":"4h0hJ9FTDlIM"},"source":["### Create a Deeper Network"]},{"cell_type":"markdown","metadata":{"id":"YmSduBmytrqb"},"source":["<span style=\"color:red\"><strong>TODO</strong></span>: Create your MLP in tensorflow. Since you are going to create a deeper neural network, it is recommended to use \"list\" to store your network parameters (weights and bias). Consider using a loop to create your MLP network.\n","\n","The model structure is described below:\n","\n","**input tensor** -> **Dense layer (relu, 100 hidden dims)** -> **Dense layer (relu, 200 hidden dims)** -> **Affine layer ( 20 hidden dims)** -> **Softmax**\n","\n","And add L2 regularization with 0.01 regularization weight when calculating the loss."]},{"cell_type":"code","source":[],"metadata":{"id":"6KTIZEeYTuOu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wuym641tPT1F"},"source":["<span style=\"color:red\"><strong>TODO</strong></span>: define the network structure"]},{"cell_type":"code","metadata":{"id":"Hjne4d-9Ngtl","executionInfo":{"status":"ok","timestamp":1730948000783,"user_tz":300,"elapsed":178,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["#Define network structure\n","input_dim = 3072\n","num_classes = 20\n","reg_tf = tf.constant(0.01) # regularization parameter\n","# %%%%%%%%%%%%%% implement your code below (3 lines) %%%%%%%%%%%%%%\n","hidden_dims = [100, 200, 20]  # list of hidden layer dimensions\n","num_layers = len(hidden_dims)  # total number of layers\n","layer_dims = [input_dim] + hidden_dims  # list of all layer dimensions including input\n","# %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yC2mC3LUQll3"},"source":["<span style=\"color:red\"><strong>TODO</strong></span>: define the network class"]},{"cell_type":"code","metadata":{"id":"a4KQsWnXP8QM","executionInfo":{"status":"ok","timestamp":1730948005134,"user_tz":300,"elapsed":171,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["import tensorflow as tf\n","\n","\n","# using Clousre\n","class Model(tf.keras.Model):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","\n","        # %%%%%%%%%%%%%% implement your code below (2 lines) %%%%%%%%%%%%%%\n","        self.W = [\n","            tf.Variable(\n","                tf.random.normal([layer_dims[i], layer_dims[i+1]], stddev=0.1),\n","                name=f'W{i}'\n","            ) for i in range(num_layers)\n","        ]\n","        self.b = [\n","            tf.Variable(\n","                tf.zeros([layer_dims[i+1]]),\n","                name=f'b{i}'\n","            ) for i in range(num_layers)\n","        ]\n","        # %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","\n","    def call(self, inputs):\n","        hidden_layers = []\n","\n","        # Add Affine layers with ReLU activation\n","        for i in range(num_layers - 1):  # iterate over each dense layer\n","            # %%%%%%%%%%%%%% implement your code below (2 lines) %%%%%%%%%%%%%%\n","            temp = tf.matmul(inputs, self.W[i]) + self.b[i]  # implement the dense layer\n","            hidden_layers.append(tf.nn.relu(temp))  # append with ReLU activation\n","            inputs = hidden_layers[-1]  # update input for next layer\n","            # %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","\n","        # Final layer\n","        # %%%%%%%%%%%%%% implement your code below (1 line) %%%%%%%%%%%%%%\n","        out = tf.matmul(inputs, self.W[-1]) + self.b[-1]  # implement final layer\n","        # %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","\n","        return tf.nn.softmax(out)  # Apply softmax to get probabilities\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"duplA2kv7EEK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWTDUOywP59M"},"source":["<span style=\"color:red\"><strong>TODO</strong></span>: define the loss function"]},{"cell_type":"code","metadata":{"id":"kTlIIfsRRtAe","executionInfo":{"status":"ok","timestamp":1730948008432,"user_tz":300,"elapsed":163,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}}},"source":["def loss(model, inputs, targets, reg = tf.constant(0.01)):\n","    L2_loss = 0.0 #initialize the loss\n","    # %%%%%%%%%%%%%% implement your code below (2 lines) %%%%%%%%%%%%%%\n","    out = model(inputs)  # get model output\n","    # Calculate cross entropy loss with softmax\n","    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","        labels=tf.cast(targets, tf.int32),\n","        logits=out\n","    )\n","    # %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","    for i in range(num_layers):\n","        # %%%%%%%%%%%%%% implement your code below (1 line) %%%%%%%%%%%%%%\n","        L2_loss += tf.reduce_sum(tf.square(model.W[i]))    #add the L2 regularization\n","        # %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","    return tf.reduce_mean(cross_entropy) + reg * L2_loss"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7ZR72tORLWY"},"source":["We can re-utilize the gradient calculation(**def grad()**), accuracy evaluation (**def eval_acc()**) function and the training loop:\n"]},{"cell_type":"code","metadata":{"id":"UlSdxbloRLmw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730948058774,"user_tz":300,"elapsed":48280,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}},"outputId":"95ec679a-9187-4059-9a93-c1f7fa7989a2"},"source":["def grad(model, inputs, targets, reg = tf.constant(0.01)):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss(model, inputs, targets, reg=reg)\n","    return tape.gradient(loss_value, (model.W + model.b))\n","\n","def eval_acc(model, inputs, targets):\n","    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    return accuracy\n","\n","num_train = 49000\n","batch_size = 500\n","num_batch = num_train//batch_size\n","num_epochs = 10\n","model = Model()\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n","\n","for e in range(num_epochs):\n","    for i in range(num_batch):\n","        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n","        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n","        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n","\n","        grads = grad(model, x_tf, y_tf, reg_tf)\n","        optimizer.apply_gradients(zip(grads, (model.W + model.b)))\n","\n","    x_tf = tf.Variable(X_val, dtype = tf.float32)\n","    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n","    accuracy = eval_acc(model, x_tf, y_tf)\n","    val_acc = accuracy.numpy()\n","    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n","\n","x_tf = tf.Variable(X_test, dtype = tf.float32)\n","y_tf = tf.Variable(y_test, dtype = tf.uint8)\n","accuracy = eval_acc(model, x_tf, y_tf)\n","test_acc = accuracy.numpy()\n","print('test acc = {}'.format(test_acc))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: valid acc = 0.05999999865889549\n","epoch 2: valid acc = 0.07800000160932541\n","epoch 3: valid acc = 0.09700000286102295\n","epoch 4: valid acc = 0.08799999952316284\n","epoch 5: valid acc = 0.10300000011920929\n","epoch 6: valid acc = 0.10899999737739563\n","epoch 7: valid acc = 0.10999999940395355\n","epoch 8: valid acc = 0.13699999451637268\n","epoch 9: valid acc = 0.15000000596046448\n","epoch 10: valid acc = 0.14100000262260437\n","test acc = 0.12919999659061432\n"]}]},{"cell_type":"markdown","metadata":{"id":"cv6uUJOqDlIN"},"source":["## Part 2: Introduction to TensorFlow.keras\n","\n","As you can see, as the network structure becomes larger it gets harder to handle variables from every layer. Here we introduce the `tf.keras` tool to build the network in a much simpler way."]},{"cell_type":"markdown","metadata":{"id":"vw_cEmWKDlIN"},"source":["<span style=\"color:red\"><strong>TODO</strong></span>: Follow this official example: https://www.tensorflow.org/datasets/keras_example#step_2_create_and_train_the_model to build and train the MLP model shown below:\n","\n","**input tensor** -> **Dense layer (relu, 100 hidden dims)** -> **Dense layer (relu, 200 hidden dims)** -> **Affine layer (20 hidden dims)** -> **Softmax**\n","\n","*You should keep the same optimizer (SGD with the same learning rate and batch size) and loss function (cross entropy with L2 regularization) as the previous task.*\n","\n","You need to study the usage of\n","[`tf.keras.layers.Dense`](https://keras.io/api/layers/core_layers/dense/) to fully equip all the functionalities that we used above.\n","\n","You need to check the usage of [`model.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and feed the model with our own data.\n","\n","As the result, it should return a similar accuracy as our above sample (~0.33 validation accuracy ). The reason why we want you to replicate the model and hyper-parameters is that you can learn almost all aspects of basic model implementation in Keras. In the future, you can freely use these useful building blocks to build your own models.\n","\n","**Tips:**\n","* Softmax is also registered as a layer operation in tf.keras.\n","* You can use `model.summary()` to visualize the model after you build it.\n","* Use `verbose=2` in `model.fit()` to get similar training logs\n","* **Keras Important API Reference:** https://keras.io/api/"]},{"cell_type":"code","metadata":{"id":"6cEr7u9xDlIO","colab":{"base_uri":"https://localhost:8080/","height":728},"executionInfo":{"status":"ok","timestamp":1730948155043,"user_tz":300,"elapsed":64584,"user":{"displayName":"Raymond Lei","userId":"09034843123563673012"}},"outputId":"b81d8c41-a71c-4fc9-b304-8831629317bd"},"source":["initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-3, seed=None) # the default initialization of Dense Layer is not random normalization\n","#############################################################\n","# TODO: build the model with tf.keras.models.Sequential\n","# %%%%%%%%%%%%%% implement your code below (1 line) %%%%%%%%%%%%%%\n","keras_model = keras_model = tf.keras.models.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(3072,)),\n","    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=initializer),\n","    tf.keras.layers.Dense(200, activation='relu', kernel_initializer=initializer),\n","    tf.keras.layers.Dense(20, kernel_initializer=initializer),\n","    tf.keras.layers.Softmax()]) # add code into the parentheses\n","# %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","#############################################################\n","\n","\n","\n","#############################################################\n","# TODO: compile the model, set optimizer and loss and metrics\n","# %%%%%%%%%%%%%% implement your code below (1 line) %%%%%%%%%%%%%%\n","keras_model.compile(\n","    optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","# %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","#############################################################\n","\n","\n","\n","#############################################################\n","# TODO: train the model with our own dataset\n","keras_model.summary()\n","\n","# train the model\n","y_train_keras = tf.one_hot(y_train, 20)\n","y_val_keras = tf.one_hot(y_val, 20)\n","# %%%%%%%%%%%%%% implement your code below (1 line) %%%%%%%%%%%%%%\n","keras_model.fit(X_train,\n","    y_train_keras,\n","    batch_size=32,\n","    epochs=10,\n","    validation_data=(X_val, y_val_keras),\n","    verbose=2) # add code into the parentheses\n","# %%%%%%%%%%%%%% your code ends here %%%%%%%%%%%%%%\n","#############################################################"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │         \u001b[38;5;34m307,300\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m20,200\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │           \u001b[38;5;34m4,020\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ softmax (\u001b[38;5;33mSoftmax\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">307,300</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,020</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m331,520\u001b[0m (1.26 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331,520</span> (1.26 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m331,520\u001b[0m (1.26 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331,520</span> (1.26 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1532/1532 - 6s - 4ms/step - accuracy: 0.0891 - loss: 2.9943 - val_accuracy: 0.0880 - val_loss: 2.9827\n","Epoch 2/10\n","1532/1532 - 4s - 3ms/step - accuracy: 0.1179 - loss: 2.7775 - val_accuracy: 0.1540 - val_loss: 2.6610\n","Epoch 3/10\n","1532/1532 - 7s - 4ms/step - accuracy: 0.2024 - loss: 2.5402 - val_accuracy: 0.1980 - val_loss: 2.5099\n","Epoch 4/10\n","1532/1532 - 4s - 3ms/step - accuracy: 0.2535 - loss: 2.3934 - val_accuracy: 0.2520 - val_loss: 2.4030\n","Epoch 5/10\n","1532/1532 - 4s - 3ms/step - accuracy: 0.2876 - loss: 2.2935 - val_accuracy: 0.2800 - val_loss: 2.3325\n","Epoch 6/10\n","1532/1532 - 6s - 4ms/step - accuracy: 0.3153 - loss: 2.2121 - val_accuracy: 0.3030 - val_loss: 2.2398\n","Epoch 7/10\n","1532/1532 - 4s - 3ms/step - accuracy: 0.3376 - loss: 2.1438 - val_accuracy: 0.3240 - val_loss: 2.2084\n","Epoch 8/10\n","1532/1532 - 6s - 4ms/step - accuracy: 0.3535 - loss: 2.0871 - val_accuracy: 0.3310 - val_loss: 2.2003\n","Epoch 9/10\n","1532/1532 - 6s - 4ms/step - accuracy: 0.3690 - loss: 2.0386 - val_accuracy: 0.3140 - val_loss: 2.1823\n","Epoch 10/10\n","1532/1532 - 10s - 7ms/step - accuracy: 0.3817 - loss: 1.9934 - val_accuracy: 0.3230 - val_loss: 2.1849\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7dbe2e6822f0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"3oO2u5q5LvLX"},"source":[],"execution_count":null,"outputs":[]}]}