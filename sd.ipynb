{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from attention import SelfAttention, CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.linear_2 = nn.Linear(4 * n_embd, 4 * n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, 320)\n",
    "\n",
    "        # (1, 320) -> (1, 1280)\n",
    "        x = self.linear_1(x)\n",
    "        \n",
    "        # (1, 1280) -> (1, 1280)\n",
    "        x = functional.silu(x) \n",
    "        \n",
    "        # (1, 1280) -> (1, 1280)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(320)\n",
    "        #self.unet = UNET()\n",
    "        #self.final = UNET_OutputLayer(320, 4)\n",
    "\n",
    "    def forward(self, latent, context, time):\n",
    "        # latent: (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        # context: (Batch_Size, Seq_Len, Dim)\n",
    "        # time: (1, 320)\n",
    "\n",
    "        # (1, 320) -> (1, 1280)\n",
    "        #time = self.time_embedding(time)\n",
    "        \n",
    "        # (Batch, 4, Height / 8, Width / 8) -> (Batch, 320, Height / 8, Width / 8)\n",
    "        #output = self.unet(latent, context, time)\n",
    "        \n",
    "        # (Batch, 320, Height / 8, Width / 8) -> (Batch, 4, Height / 8, Width / 8)\n",
    "        #output = self.final(output)\n",
    "        \n",
    "        # (Batch, 4, Height / 8, Width / 8)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
